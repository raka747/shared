{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python_Data_Science_2(Text Mining)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Latent Semantic Analysis: Dimensionality Reduction on Text; helps in interpretation of data and on computation time.\n",
    "\n",
    "NLP is very good at\n",
    "    Spam detection\n",
    "    POS(parts-of-speech) tagging\n",
    "    NER (named-entity recognition) : identifies which part of sentence is Person/Location/Organization e.t.c\n",
    "    \n",
    "NLP is pretty good but not great\n",
    "    sentiment analysis\n",
    "    machine translation : ex: language1 -> language2 -> language3 -> language1 (language1 != language1)\n",
    "    information extraction : IOS extracting info from email to add to calender\n",
    "    \n",
    " NLP needs improvement\n",
    "     Machine conversations (recognize speech (siri and Alexa) ex: 'recognize speech' almost the same as 'wreck a nice beach')\n",
    "     Paraphrasing nd summarization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Yet to Learn\n",
    "Sparse Matrices in Python: https://machinelearningmastery.com/sparse-matrices-for-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:/Users/rpothams/Downloads/BD/NLP/nlp_class/spambase.data', engine='python', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac = 1).reset_index(drop = True)\n",
    "Xtrain = data.iloc[:-100,:-1]\n",
    "Ytrain = data.iloc[:-100,-1]\n",
    "\n",
    "Xtest = data.iloc[-100:,:-1]\n",
    "Ytest = data.iloc[-100:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = pd.read_csv('spambase.data').values # use pandas for convenience # gives a matrix\n",
    "np.random.shuffle(data) #inplace shuffling\n",
    "or\n",
    "data = pd.read_csv('spambase.data').as_matrix() # use pandas for convenience # gives a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification rate for NB: 0.79\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "model = MultinomialNB()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print(\"Classification rate for NB:\", model.score(Xtest, Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification rate for Adaboost: 0.79\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost\n",
    "model2 = AdaBoostClassifier()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print(\"Classification rate for Adaboost:\", model.score(Xtest, Ytest))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SPAM Detection Example II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_df = pd.read_csv('C:/Users/rpothams/Downloads/BD/NLP/spam.csv', encoding= 'iso-8859-1')\n",
    "spam_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_df = spam_df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "spam_df.columns = ['labels', 'data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating binary labels\n",
    "spam_df['b_labels'] = spam_df['labels'].map({'ham': 0, 'spam': 1})\n",
    "Y = spam_df['b_labels'].values #numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = spam_df['data']\n",
    "# using CountVectorizer\n",
    "count_vect = CountVectorizer(decode_error='ignore')\n",
    "X_count = count_vect.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "# fit and then transform\n",
    "tfidf.fit(sentences)\n",
    "X_tfidf = tfidf.transform(sentences) #gives a numpy sparse matrix\n",
    "#both fit and transform in one step\n",
    "#X = tfidf.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X_tfidf, Y, test_size = 0.33)\n",
    "Xtrain2, Xtest2, Ytrain2, Ytest2 = train_test_split(X_count, Y, test_size = 0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4218)\t1\n",
      "  (0, 7621)\t1\n",
      "  (0, 2689)\t1\n",
      "  (0, 8330)\t1\n",
      "  (0, 984)\t1\n",
      "  (0, 7659)\t1\n",
      "  (0, 2214)\t1\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.7645325475488883\n",
      "test score: 0.7792278412180533\n"
     ]
    }
   ],
   "source": [
    "model_countV = MultinomialNB()\n",
    "model_countV.fit(Xtrain2, Ytrain2)\n",
    "print(\"train score:\", model_countV.score(Xtrain2, Ytrain))\n",
    "print(\"test score:\", model_countV.score(Xtest2, Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9649075810340209\n",
      "test score: 0.9575856443719413\n"
     ]
    }
   ],
   "source": [
    "model_tfidf = MultinomialNB()\n",
    "model_tfidf.fit(Xtrain, Ytrain)\n",
    "print(\"train score:\", model_tfidf.score(Xtrain, Ytrain))\n",
    "print(\"test score:\", model_tfidf.score(Xtest, Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud\n",
    "from wordcloud import WordCloud\n",
    "def visualize(df, label):\n",
    "    words = ''\n",
    "    for msg in df[df.iloc[:,1] == label].iloc[:,0]:\n",
    "        msg = msg.lower()\n",
    "        words += msg + ' '\n",
    "    wordcloud = WordCloud(width=600, height=400).generate(words)\n",
    "    plt.imshow(worcloud) #draws an image on the current figure (creating a figure is there isn't a current figure)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-*-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Section 3 - Psentiment Analysis\n",
    "XML Parser - BeautifulSoup\n",
    "\n",
    "#### Tokenization\n",
    "why not just use .split for tokenization?\n",
    "    contractions - combination of multiple words\n",
    "        don't = do not\n",
    "        won't = would not\n",
    "        she'll = she will\n",
    "    Extract punctuations and symbols: Period (.) and Dollar ($)\n",
    "    This is all done by nltk.tokenize.word_tokenize\n",
    "\n",
    "Custom Tokenizer (to perform better than in built nltk)\n",
    "    - remove stopwords\n",
    "    - remove numbers and single-letter tokens\n",
    "    - application specific\n",
    "    \n",
    "#### Tokens into Vectors\n",
    "    - scikitLear has it inbuilt\n",
    "    - lets write it manually\n",
    "    - each word is a key and index is a value - Dictionary/map\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the word is present, turn the value to 1, irresepective of frequency\n",
    "V = 2000 #Vocab size\n",
    "x = np.zeros(V)\n",
    "for token in sentence:\n",
    "    index = word_index_map[token]\n",
    "    x[index] = 1\n",
    "    #x[index] += 1 # for raw counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-8b0f42a80593>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# positive.review is a XML file with Amazon positive reviews\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mpositive_reviews\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/rpothams/Downloads/BD/NLP/nlp_class/electronics/positive.review'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'xml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mpositive_reviews\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpositive_reviews\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'review_text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mnegative_reviews\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/rpothams/Downloads/BD/NLP/nlp_class/electronics/negative.review'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "# For sentiment analysis, we'll use raw counts, but qhat's the problem?\n",
    "# Really long docs become really long vectors, really short docs have short vectors (ex: amazon long and short reviews)\n",
    "# classifier have to deal with vectors of varying lengths and we dont want comparison based on length of vectors, hnce normalize\n",
    "# Vector/# of words in document\n",
    "\n",
    "# Another reason for normalization\n",
    "# LogisticRegression performs better with normalizaed data (sigmoid(10) ~= sigmoid(100) ~= 1)\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer # turns words in to base forms i.e dogs -> dog\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stopwords = set(w.rstrip() for w in open('C:/Users/rpothams/Downloads/BD/NLP/nlp_class/stopwords.txt'))\n",
    "\n",
    "# positive.review is a XML file with Amazon positive reviews\n",
    "positive_reviews = BeautifulSoup(open('C:/Users/rpothams/Downloads/BD/NLP/nlp_class/electronics/positive.review'), 'xml').read()\n",
    "positive_reviews = positive_reviews.findall('review_text')\n",
    "negative_reviews = BeautifulSoup(open('C:/Users/rpothams/Downloads/BD/NLP/nlp_class/electronics/negative.review')).read()\n",
    "negative_reviews = negative_reviews.findall('review_text')\n",
    "\n",
    "np.random.shuffle(positive_reviews)\n",
    "#positive_reviews = positive_reviews[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='C:/Users/rpothams/Downloads/BD/NLP/nlp_class/electronics/positive.review' mode='r' encoding='cp1252'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open('C:/Users/rpothams/Downloads/BD/NLP/nlp_class/electronics/positive.review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
