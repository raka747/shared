{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python_Data_Science_2(Text Mining)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Latent Semantic Analysis: Dimensionality Reduction on Text; helps in interpretation of data and on computation time.\n",
    "\n",
    "NLP is very good at\n",
    "    Spam detection\n",
    "    POS(parts-of-speech) tagging\n",
    "    NER (named-entity recognition) : identifies which part of sentence is Person/Location/Organization e.t.c\n",
    "    \n",
    "NLP is pretty good but not great\n",
    "    sentiment analysis\n",
    "    machine translation : ex: language1 -> language2 -> language3 -> language1 (language1 != language1)\n",
    "    information extraction : IOS extracting info from email to add to calender\n",
    "    \n",
    " NLP needs improvement\n",
    "     Machine conversations (recognize speech (siri and Alexa) ex: 'recognize speech' almost the same as 'wreck a nice beach')\n",
    "     Paraphrasing nd summarization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Yet to Learn\n",
    "Sparse Matrices in Python: https://machinelearningmastery.com/sparse-matrices-for-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6cd184213edd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\wordcloud\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m from .wordcloud import (WordCloud, STOPWORDS, random_color_func,\n\u001b[0m\u001b[0;32m      2\u001b[0m                         get_single_color_func)\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcolor_from_image\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImageColorGenerator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m __all__ = ['WordCloud', 'STOPWORDS', 'random_color_func',\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0moperator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mitemgetter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImageColor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImageDraw\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;31m# Also note that Image.core is not a publicly documented interface,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;31m# and should be considered private and subject to change.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_imaging\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mPILLOW_VERSION\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'PILLOW_VERSION'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         raise ImportError(\"The _imaging extension was built for another \"\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:/Users/rpothams/Downloads/BD/NLP/nlp_class/spambase.data', engine='python', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac = 1).reset_index(drop = True)\n",
    "Xtrain = data.iloc[:-100,:-1]\n",
    "Ytrain = data.iloc[:-100,-1]\n",
    "\n",
    "Xtest = data.iloc[-100:,:-1]\n",
    "Ytest = data.iloc[-100:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = pd.read_csv('spambase.data').values # use pandas for convenience # gives a matrix\n",
    "np.random.shuffle(data) #inplace shuffling\n",
    "or\n",
    "data = pd.read_csv('spambase.data').as_matrix() # use pandas for convenience # gives a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "model = MultinomialNB()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print(\"Classification rate for NB:\", model.score(Xtest, Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost\n",
    "model2 = AdaBoostClassifier()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print(\"Classification rate for Adaboost:\", model.score(Xtest, Ytest))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SPAM Detection Example II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_df = pd.read_csv('C:/Users/rpothams/Downloads/BD/NLP/spam.csv', encoding= 'iso-8859-1')\n",
    "spam_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_df = spam_df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "spam_df.columns = ['labels', 'data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating binary labels\n",
    "spam_df['b_labels'] = spam_df['labels'].map({'ham': 0, 'spam': 1})\n",
    "Y = spam_df['b_labels'].values #numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = spam_df['data']\n",
    "# using CountVectorizer\n",
    "count_vect = CountVectorizer(decode_error='ignore')\n",
    "X_count = count_vect.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "# fit and then transform\n",
    "tfidf.fit(sentences)\n",
    "X_tfidf = tfidf.transform(sentences) #gives a numpy sparse matrix\n",
    "#both fit and transform in one step\n",
    "#X = tfidf.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X_tfidf, Y, test_size = 0.33)\n",
    "Xtrain2, Xtest2, Ytrain2, Ytest2 = train_test_split(X_count, Y, test_size = 0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Xtrain2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_countV = MultinomialNB()\n",
    "model_countV.fit(Xtrain2, Ytrain2)\n",
    "print(\"train score:\", model_countV.score(Xtrain2, Ytrain))\n",
    "print(\"test score:\", model_countV.score(Xtest2, Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tfidf = MultinomialNB()\n",
    "model_tfidf.fit(Xtrain, Ytrain)\n",
    "print(\"train score:\", model_tfidf.score(Xtrain, Ytrain))\n",
    "print(\"test score:\", model_tfidf.score(Xtest, Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud\n",
    "from wordcloud import WordCloud\n",
    "def visualize(df, label):\n",
    "    words = ''\n",
    "    for msg in df[df.iloc[:,1] == label].iloc[:,0]:\n",
    "        msg = msg.lower()\n",
    "        words += msg + ' '\n",
    "    wordcloud = WordCloud(width=600, height=400).generate(words)\n",
    "    plt.imshow(worcloud) #draws an image on the current figure (creating a figure is there isn't a current figure)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-*-**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
